---
title: 'Final Project: Used Pickup Truck Price Prediction'
authors: "Aaron Botelho (botelho3), Shiyu Li (shiyuli2), Steven Johnson (stevenj4), Li Li (lil6)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
# TODO(anyone) - do we need to include install.packages commands e.g. tidyverse. 
library(knitr)
library(readr)
library(broom)
library(Metrics)
opts_chunk$set(cache = TRUE, autodep = TRUE)
source("Appendix/utility_functions.R")
source("Appendix/data_cleanup.R")
```

###### Aaron Botelho (botelho3), Shiyu Li (shiyuli2), Steven Johnson (stevenj4), Li Li (lil6)
```{r import_cleaned_data, include = FALSE}
set.seed(12345)
# TODO(anyone) - How should we import the csv data without adding it to the git repo?
# For now just copy the .csv into your local git directory?
truck_df = readr::read_csv("used_truck_data_with_region.csv")
spec(truck_df)

# TODO(anyone) - this should be replaced with the relevant parts of Appendix/data_cleanup.R.
# For now since I'm using used_truck_data_with_region.csv part of data_cleanup.R
# is already done.
# Edit - hmm the factorization part seems to be in set_data_structure() fn. Maybe 
# this works.
truck_df = set_data_structure(truck_df)
truck_df[truck_df[, "mileage"] < 1.0, "mileage"] = 1.0

# A list of all columns that are _not_ characters or redundant columns
# (city, lat, long) in the data frame. The vector of column names can be used
# in formulas
no_char_columns = c(
  #X1 = col_double(),
  #dealer_zip = col_character(),
 "back_legroom" ,
  #bed" ,
  "bed_length" ,
  #cabin" ,
  #city" ,
  "city_fuel_economy" ,
  "daysonmarket" ,
  "engine_cylinders" ,
  "engine_displacement" ,
  "engine_type" ,
  "exterior_color" ,
  "fleet" ,
  "frame_damaged" ,
  "franchise_dealer" ,
  "front_legroom" ,
  "fuel_tank_volume" ,
  "fuel_type" ,
  "has_accidents" ,
  "height" ,
  "highway_fuel_economy" ,
  "horsepower" ,
  "interior_color" ,
  "isCab" ,
  "is_new" ,
  "latitude" ,
  "length" ,
  #listed_date = col_date(format = ""),
  #listing_color" ,
  "longitude" ,
  #make_name" ,
  "maximum_seating" ,
  "mileage" ,
  #model_name" ,
  "owner_count" ,
  "salvage" ,
  #"savings_amount" ,
  "seller_rating" ,
  "theft_title" ,
  "transmission" ,
  #transmission_display" ,
  "wheel_system" ,
  #wheel_system_display" ,
  "wheelbase" ,
  "width" ,
  "year" ,
  "power_hp" ,
  "power_rpm" ,
  "torque_lb_ft" ,
  "torque_rpm" ,
  "region" ,
  "state"
)
```

---

### Introduction
In this statistical study we have conducted an analysis of used pickup truck prices as a response to changes in variable data collected from scraping [Cargurus](https://www.cargurus.com/) inventory in September 2020. The dataset, which was pulled from [Kaggle](https://www.kaggle.com/ananaymital/us-used-cars-dataset), contains observations from the US used car market and was subset to isolate data by `body_type == Pickup Truck`. It was further tailored to highlight relevant variables that provide insights toward predicting our response variable, `price`.

The impetus for our carrying out this analysis was COVID-19. During the pandemic, with unprecedented migration between urban and rural centers, the prices of used vehicles experienced a notable surge. Additionally, the inventory of said vehicles declined in the US, up to 20% year-over-year in some regions. This piqued our interest as a great opportunity to explore significant indicators influencing used pickup truck prices throughout the states.

The prepared dataset is comprised of the following variables:

|Name   |Type   |Description   |
|:------|:------|:-------------|
|`back_legroom` |`num` |Legspace for backseat passengers in inches |
|`bed` |`Factor w/ 4 levels` |Type of truck bed (Long, Short, etc.) |
|`bed_length` |`num` |Length of truck bed in inches |
|`cabin` |`Factor w/ 5 levels`|Type of passenger cabin (Crew Cab, Extended Cab, etc.) |
|`city` |`Factor w/ 4060 levels` |City where vehicle is being sold |
|`city_fuel_economy` |`num` |MPG mileage in city |
|`daysonmarket` |`num` |Number of days listed for sale |
|`dealer_zip` |`Factor w/ 6897 levels` | Zip code of dealer |
|`engine_cylinders` |`Factor w/ 4 levels` |Number of engine cylinders (4, 6, 8, etc.) |
|`engine_displacement` |`num` |Engine displacement volume |
|`engine_type` |`Factor w/ 4 levels` |Number of engine cylinders (4, 6, 8, etc.) |
|`exterior_color` |`Factor w/ 10 levels` |Exterior color of vehicle |
|`fleet` |`logi` |- - - |
|`frame_damaged` |`logi` |Indicator for frame damage |
|`franchise_dealer` |`logi` |- - - |
|`front_legroom` |`num` |Legspace for frontseat passengers in inches |
|`fuel_tank_volume` |`num` |Capacity of the fuel tank |
|`fuel_type` |`Factor w/ 5 levels` |Type of fuel the vehicle uses to operate |
|`has_accidents` |`logi` |Indicator for whether or not the vehicle has been in an accident |
|`height` |`num` |Height of vehicle in inches |
|`highway_fuel_economy` |`num` |MPG mileage in highway |
|`horsepower` |`num` |Vehicle horsepower |
|`interior_color` |`Factor w/ 10 levels` |Interior color of vehicle |
|`isCab` |`logi` |- - - |
|`is_new` |`logi` |Indicator for a new car |
|`latitude` |`num` |Geographic latitude |
|`length` |`num` |Length of the vehicle in inches |
|`listed_date` |`chr` |Date vehicle was listed for sale |
|`listing_color` |`Factor w/ 10 levels` |Exterior color of vehicle |
|`longitude` |`num` |Geographic longitude |
|`make_name` |`Factor w/ 17 levels` |Manufacturer name |
|`maximum_seating` |`num` |Maximum number of seats in vehicle |
|`mileage` |`num` |Odometer reading |
|`model_name` |`chr` |Model of vehicle from manufacturer |
|`owner_count` |`num` |Number of previous owners |
|`price` |`num` |Sale price of used vehicle (Response) |
|`salvage` |`logi` |Indicator for vehicle being salvage |
|`savings_amount` |`num` |- - - |
|`seller_rating` |`num` |Consumer rating of vehicle seller |
|`theft_title` |`logi` |- - - |
|`transmission` |`Factor w/ 2 levels`|Automatic or Manual transmission (A or M) |
|`transmission_display` |`chr` |Transmission display within vehicle |
|`wheel_system` |`Factor w/ 5 levels` |Vehicle wheel-system (4WD, 4X2, 2WD, etc.) |
|`wheel_system_display` |`chr` |Vehicle wheel-system display within vehicle |
|`wheelbase` |`num` |Wheelbase width in inches |
|`width` |`num` |Width of vehicle in inches |
|`year` |`num` |Manufactured year of vehicle |
|`power_hp` |`num` |- - - |
|`power_rpm` |`num` |- - - |
|`torque_lb_ft` |`num` |Vehicle torque in foot pounds |
|`torque_rpm` |`num` |Vehicle torque RPM |
|`region` |`Factor w/ 9 levels` |US region of vehicle seller |
|`state` |`Factor w/ 49 levels` |State of vehicle seller |

In creating a linear regression model around these variables, we aim to (not only) identify the significant predictors of price for used pickup trucks, but to seek and potentially offer strategies for consumers to achieve the best value for their money.

---

### Methods
```{r}
# TODO
```

#### Data Cleaning
```{r}
# TODO
```

#### Naive Models

```{r naive_hypothesis_model, echo=TRUE, results = 'hide'}

```

Before continuing with analysis of our data we fit a naive large additive model
using all numeric and factor predictors with no interactions. Since $R^2$
increases and $RMSE$ decreases as more predictors are added, this additive model
should give us a good fit to the data. Many of the predictors will likely be
redundant.

Regardless this model will provide a baseline against which we can compare more
optimized models. It can also be used for analyzing the impact of predictor/response
transformations or used for outlier analysis.

```{r mega_model, echo=TRUE}
# Create a formual string for a simple additive model using all non-character columns
mega_model_formula = formula(paste("price ~", paste("", no_char_columns, sep = " ", collapse = " +")))
mega_model = lm(mega_model_formula, data = truck_df)
#summary(mega_model)
print_useful_summary_stats(mega_model)
```



#### Outlier Analysis

After cleaning the trucks dataframe our next step in building the pricing model
is to understand the distribution and correlations in out dataset. Histograms
are used to determine the distribution of a few columns we expect will be key to
the final model.

Pairs plots and covariance functions can be used to determine
the relationship of predictors to each other, and to the response. Any
predictors with linear relationships in the pairs plot are collinear and will
likely be redundant in an optimal model. The scatter plots of each predictor
against the response variable in the pairs plot provides a rough indication of
which predictors have a linear, quadratic, or logarithmic relationship with the
response variable.

After determining the column's colinearity and distribution we turn our attention
to the rows. Our dataset is scraped from a website with user-generated input and
has not been filtered. There are certain to be many rows with user errors, e.g
a 10,000$ car being entered as 100,000\$, or a 4 cylinder car being mistakenly
listed as 6 cylinders. We cannot manually search the data frame for such
inconsistencies but we can use outlier analysis, and the standardized residual 
to discover rows that are many standard deviations from the norm and our model.

To perform the outlier analysis we will use a simple additive model using most
predictors, with a high R^2, low p-value and reasonable standard error. This 
simple fit allows us to generate residuals and use tools like rstandard and 
Cook's Distance.

```{r histogram_analysis, echo=FALSE}
par(mfrow=c(3,2))
hist(truck_df$year, breaks=length(unique(truck_df$year)),
     xlab="Year", ylab = "Frequency", main = "Histogram of Model Year")
hist(as.integer(truck_df$state), breaks=length(unique(truck_df$state)),
     xlab="State", ylab = "Frequency", main = "Histogram of State")
hist(truck_df$horsepower, breaks=30,
     xlab="Horsepower", ylab = "Frequency", main = "Histogram of Horsepower")
hist(log(truck_df$mileage), breaks=30,
     xlab="log(Mileage)", ylab = "Frequency", main = "Histogram of log(Mileage)")
hist(as.integer(truck_df$maximum_seating), breaks=30,
     xlab="Maximum Seating", ylab = "Frequency", main = "Histogram of Max seating")
```


```{r colinearity_analysis, echo=FALSE}
# TODO(aaronbotelho, li)
pairs_columns = c("price", "engine_cylinders", "engine_displacement", "mileage", "daysonmarket", "year", "listed_date")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns]
pairs(pairs_df, col = "dodgerblue")
```

Notes from Pairs plot. 

- The relationship of price to mileage looks like it might be better modeled by
1/x. Log(x) would probably work too because of the orders of magnitude.
- engine_cylinders and engine_displacement are collinear. Let's use
engine_displacement since it's an integer not a factor.
- year is almost perfectly linear + quadratic with price.
- daysonmarket is nor very useful lots of noise

```{r colinearity_analysis_2, echo=FALSE}
pairs_columns2 = c("price", "wheel_system", "power_hp", "torque_lb_ft", "maximum_seating", "owner_count")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns2]
pairs(pairs_df, col = "darkorange")
```

Notes from Pairs plot.

- power_hp and torque_lb_ft are colinear
- maximum_seating is strongly linear with price.
- wheel system is also strongly linear 

```{r colinearity_analysis_3, echo=FALSE}
pairs_columns2 = c("price", "latitude", "longitude")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns2]
pairs(pairs_df, col = "darkorange")
```

Notes from pairs plot:

- Both latitude and longitude have a linear relationship with price, a
horizontal line.
- The variance in price vs. latitude or longitude is constant.
- Latitude and longitude should not be useful predictors of price.
- The scatter plot of latitude vs. longitude forms a map of the United States. 


```{r colinearity_analysis_4}
#TODO(anyone) - vif analysis
```



```{r colinearity_analysis_last, include=FALSE}
free_memory(pairs_df)
```

Next we attempt to identify outliers, observations which do not fit the model well,
and which may have significant impact on the coefficients of the model. Often
these observations will often have large residuals.

```{r outlier_analysis_1, results = 'hide'}
# Create a formula tring for a simple additive model using all non-character columns
columns_of_interest = setdiff(no_char_columns, c("latitude", "longitude"))
no_char_additive_formula = formula(paste("price ~", paste("", columns_of_interest, sep = " ", collapse = " +")))
#rstandard 
#cooks.distance 
#2 Naive Models (mega model and “common” model) - Aaron or Anyone 
ofit0 = lm(no_char_additive_formula, data = truck_df)
#summary(ofit0)
count_gt_stddev = c(length(which(rstandard(ofit0) > 1)),
  length(which(rstandard(ofit0) > 2)),
  length(which(rstandard(ofit0) > 3)),
  length(which(rstandard(ofit0) > 4)))
number_of_stddevs = c(1,2,3,4)
model_results = data.frame(number_of_stddevs, count_gt_stddev)
colnames(model_results) = c("# of stddev", "Count of rows > stddev")
knitr::kable(model_results)

# Do some Cook's distance tests. The intersection of high-outlier, 
# high-influence is 95%. Let's remove these from the dataset.
high_influence_rows = which(cooks.distance(ofit0) > 4/length(ofit0$residuals))
intersection_of_high_inf_high_outlier = intersect(which(rstandard(ofit0) > 4), high_influence_rows)
length(intersection_of_high_inf_high_outlier) / count_gt_stddev[4] 

# Let's take a look at the outlier rows to see if there's any commonality.
outlier_rows_to_remove = intersection_of_high_inf_high_outlier 
outlier_df = truck_df[outlier_rows_to_remove, ]
```



```{r outlier_analysis_2, echo=TRUE, results='hide'}
# Let's take a look at the outlier rows to see if there's any commonality.
outlier_rows_to_remove = intersection_of_high_inf_high_outlier 
outlier_df = truck_df[outlier_rows_to_remove, ]
```

```{r outlier_analysis_3, echo=FALSE}
par(mfrow=c(2,2))
hist(outlier_df$year, breaks=length(unique(outlier_df$year)),
     xlab="Year", ylab = "Frequency", main = "Histogram of Model Year")
hist(as.integer(outlier_df$state), breaks=length(unique(outlier_df$state)),
     xlab="State", ylab = "Frequency", main = "Histogram of State")
hist(log(outlier_df$mileage), breaks=30,
     xlab="log(Mileage)", ylab = "Frequency", main = "Histogram of log(Mileage)")

# There's a modest improvement in R2 0.01 or 1%. The std error drops ~100. The
# fitted vs. residuals plot becomes waaaay more useful.
#ofit1 = lm(price ~ year + I(year^2) + engine_displacement + mileage + I(mileage^2) +
#   maximum_seating + wheel_system + power_hp + bed_length +
#  frame_damaged + salvage + state,
#   data = truck_df[-outlier_rows_to_remove, ])
#summary(ofit1)
#plot_fitted_resid(ofit1)
#
#ofit2 = lm(price ~ year + I(year^2) + engine_displacement + log(mileage) +
#             I(mileage^2) + mileage +
#   maximum_seating + wheel_system + power_hp + bed_length +
#  frame_damaged + salvage + state,
#   data = truck_df[-outlier_rows_to_remove, ])
#summary(ofit2)
#anova(ofit1, ofit2)
```

Re-fit the mega_model with the outlier observations removed. The outlier rows
we removed were very influential based on their rstandard and Cook's distance
results. See if the beta coefficients change significantly or if the RMSE
changes.

```{r outlier_analysis_4, include=FALSE}
#formula_str = formula(paste("price ~", paste("", no_char_columns, sep = " ", collapse = " +")))
#formula_str = update.formula(formula_str,  ~ . + log(mileage))
ofit2_no_outliers = lm(mega_model_formula, data = truck_df[-outlier_rows_to_remove,])
#summary(ofit2_no_outliers)
print_useful_summary_stats(ofit2_no_outliers)
plot_fitted_resid(ofit2_no_outliers)
# Percentage change in model coefficients.
abs((coef(ofit2_no_outliers) - coef(mega_model))/coef(ofit2_no_outliers)) * 100
#anova(mega_model, ofit2_no_outliers)
```

It's clear from the code segment above that the RMSE and $R^2$ value of the
model is mostly unaffected by removal of the outlier rows. However there is a large
percentage change ~3%-9% in some of the model beta coefficients when refitting
the model.


```{r outlier_analysis_last, include=FALSE}
free_memory(ofit1)
free_memory(ofit2)
free_memory(fit2_additive_all)
```

#### Discussion of Per - Region Models
```{r}
# Helper functions
#plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
#  plot(fitted(model), resid(model), 
#       col = pointcol, pch = 20, cex = 1.5,
#       xlab = "Fitted", ylab = "Residuals")
#  abline(h = 0, col = linecol, lwd = 2)
#}
#
#plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
#  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
#  qqline(resid(model), col = linecol, lwd = 2)
#}
#
#cal_rmse = function(model){
#  sqrt(mean(resid(model) ^ 2))
#}
#calc_loocv_rmse = function(model) { 
#   sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
#}
```

```{r}
# Sherry’s Analysis + Discussion (including model per region, as it’s significant
# – but creates add’l work -- include sample of regions) - Sherry 
# Plots: F v R, QQ, normality hist (?) 
# Target metrics 
# bp & shapiro (improvement?) 

# Modeling for mid atlantic
cols_to_keep = append(intersect(no_char_columns, colnames(truck_df)), "price")
car_data = truck_df[, cols_to_keep]
mid_altantic_data = car_data[car_data$region == "Mid Atlantic",]
drops = c("region")
mid_altantic_data = mid_altantic_data[ , !(names(mid_altantic_data) %in% drops)]
# base model
base_altantic_model = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + #cabin
                           + salvage + franchise_dealer, data = mid_altantic_data)
summary(base_altantic_model)
calc_loocv_rmse(base_altantic_model)
cal_rmse(base_altantic_model)
```

```{r}
# base model + state, state is significant
base_altantic_model_state = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + #cabin
                                 + salvage + franchise_dealer + state, data = mid_altantic_data)
summary(base_altantic_model_state)
calc_loocv_rmse(base_altantic_model_state)
cal_rmse(base_altantic_model_state)
anova(base_altantic_model, base_altantic_model_state)
```

```{r}
# mega model
mega_midat_model = lm(price~., data =mid_altantic_data)
calc_loocv_rmse(mega_midat_model)
cal_rmse(mega_midat_model)
```

```{r}
# bic_model 
n = length(resid(mega_midat_model))
model_bic_midat = step(mega_midat_model, direction = "backward", k = log(n), trace = 0)
summary(model_bic_midat)
calc_loocv_rmse(model_bic_midat)
cal_rmse(model_bic_midat)
```

```{r}
improved_midat_model= lm(formula = price ~ back_legroom + bed_length  + 
    daysonmarket + franchise_dealer + fuel_tank_volume + 
    fuel_type + height + highway_fuel_economy + 
    horsepower + isCab + is_new + longitude + 
    maximum_seating + mileage + seller_rating + 
    wheel_system + wheelbase + width + 
    year  + torque_lb_ft + torque_rpm + state, data = mid_altantic_data)
summary(improved_midat_model)
calc_loocv_rmse(improved_midat_model)
cal_rmse(improved_midat_model)
```


#### ANOVA Analysis of Global Model
```{r}
#ANOVA analysis to reduce predictors (global) - Sherry 

```

#### Variable Transformations
```{r}
#Li wizardry with the log(price) transformation - Li 

```


---

### Results
```{r}
# Run prediction on Test data and present result analysis 

```

#### Significant Plots & Things
```{r}
# TODO
```

#### Non-Significant Plots & Things
```{r}
# TODO
```

---

### Discussion
```{r}
# TODO
```
 
---

### Appendix
```{r}
# TODO
```

***
