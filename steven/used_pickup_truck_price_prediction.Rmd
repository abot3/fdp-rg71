---
title: 'Final Project: Used Pickup Truck Price Prediction'
authors: "Aaron Botelho (botelho3), Shiyu Li (shiyuli2), Steven Johnson (stevenj4), Li Li (lil6)"
date: '08/07/2021'
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
library(readr)
library(broom)
library(Metrics)
library(tidyverse)
library(stringr)
library(dplyr)
library(faraway)
library(lmtest)
library(MASS)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

###### Authors: Aaron Botelho (botelho3), Shiyu Li (shiyuli2), Steven Johnson (stevenj4), Li Li (lil6)
```{r import_cleaned_data, include=FALSE}
set.seed(12345)
truck_df = readr::read_csv("used_truck_data_with_region.csv")
spec(truck_df)

# set global functions for knitting
# set dataset structure
set_data_structure = function(dataset) {
  # update variable structure
  dataset$back_legroom = as.numeric(dataset$back_legroom)
  dataset$bed = as.factor(dataset$bed)
  dataset$bed_length = as.numeric(dataset$bed_length)
  dataset$cabin = as.factor(dataset$cabin)
  dataset$city = as.factor(dataset$city)
  # dataset$dealer_zip = as.factor(dataset$dealer_zip)
  dataset$engine_cylinders = as.factor(as.numeric(dataset$engine_cylinders))
  dataset$engine_type = as.factor(as.numeric(dataset$engine_type))
  dataset$exterior_color = as.factor(dataset$exterior_color)
  dataset$front_legroom = as.numeric(dataset$front_legroom)
  dataset$fuel_tank_volume = as.numeric(dataset$fuel_tank_volume)
  dataset$fuel_type = as.factor(dataset$fuel_type)
  dataset$height = as.numeric(dataset$height)
  dataset$interior_color = as.factor(dataset$interior_color)
  dataset$length = as.numeric(dataset$length)
  dataset$listing_color = as.factor(dataset$listing_color)
  dataset$make_name = as.factor(dataset$make_name)
  dataset$maximum_seating = as.factor(dataset$maximum_seating)
  dataset$region = as.factor(dataset$region)
  dataset$state = as.factor(dataset$state)
  dataset$transmission = as.factor(dataset$transmission)
  dataset$wheel_system = as.factor(dataset$wheel_system)
  dataset$wheelbase = as.numeric(dataset$wheelbase)
  dataset$width = as.numeric(dataset$width)
  dataset$power_hp = as.numeric(dataset$power_hp)
  dataset$power_rpm = as.numeric(dataset$power_rpm)
  dataset$torque_lb_ft = as.numeric(dataset$torque_lb_ft)
  dataset$torque_rpm = as.numeric(dataset$torque_rpm)
  dataset$year = as.numeric(dataset$year)
  dataset
}

# plot fitted vs residuals
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange", main="Fitted vs. Residuals") {
  plot(fitted(model), resid(model),
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals", main=main)
  abline(h = 0, col = linecol, lwd = 2)
}

# Q-Q plot
plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange", main = "Normal Q-Q Plot") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5, main=main)
  qqline(resid(model), col = linecol, lwd = 2)
}

# calculate RMSE
cal_rmse = function(model){
  sqrt(mean(resid(model) ^ 2))
}

# calculate leave one out cross validation
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# delete large object obj, call the garbage collector to trigger garbage collector
# to clean up deleted obj
free_memory = function(obj = data.frame()) {
  rm(obj)
  gc()
}

# select a % of random rows in the data frame - rows are chosen using a
# uniform distribution
random_rows_from_df = function(df = data.frame(), pct = 0.10) {
  rowidx = as.integer(runif(round(nrow(df) * 0.10), min = 0, max = nrow(df)))
  df[rowidx, ]
}

# calculate and display useful model summary statistics
print_useful_summary_stats = function(fit) {
  s = summary(fit)
  d = c(
    resid_std_err = c(s$sigma),
    r.squared = c(s$r.squared),
    fstatistic = c(s$fstatistic[1]),
    p.value = c(pf(s$fstatistic[1], df1=s$df[1] - 1, df2=s$df[2], lower.tail = FALSE))
  )
  d
}

# display model statistics and diagnostic information
comparison_stats = function(fit) {
  s = summary(fit)
  d = c(
    predictor_numbers = length(fit$coefficients),
    r.squared = c(s$r.squared),
    loocv_rmse = calc_loocv_rmse(fit),
    rmse = cal_rmse(fit)
  )
  d
}

truck_df = set_data_structure(truck_df)

# A list of all columns that are _not_ characters or redundant columns
# (city, lat, long) in the data frame. The vector of column names can be used
# in formulas
no_char_columns = c(
 "back_legroom" ,
  "bed_length" ,
  "cabin" ,
  "city_fuel_economy" ,
  "daysonmarket" ,
  "engine_cylinders" ,
  "engine_displacement" ,
  "engine_type" ,
  "exterior_color" ,
  "fleet" ,
  "frame_damaged" ,
  "franchise_dealer" ,
  "front_legroom" ,
  "fuel_tank_volume" ,
  "fuel_type" ,
  "has_accidents" ,
  "height" ,
  "highway_fuel_economy" ,
  "horsepower" ,
  "interior_color" ,
  "isCab" ,
  "is_new" ,
  "latitude" ,
  "length" ,
  "longitude" ,
  "maximum_seating" ,
  "mileage" ,
  "owner_count" ,
  "salvage" ,
  "seller_rating" ,
  "theft_title" ,
  "transmission" ,
  "wheel_system" ,
  "wheelbase" ,
  "width" ,
  "year" ,
  "power_hp" ,
  "power_rpm" ,
  "torque_lb_ft" ,
  "torque_rpm" ,
  "region" ,
  "state"
)
```

---

## Introduction
In this statistical study we have conducted an analysis of used pickup truck prices as a response to changes in variable data collected from scraping [Cargurus](https://www.cargurus.com/) inventory in September 2020. The dataset, which was pulled from [Kaggle](https://www.kaggle.com/ananaymital/us-used-cars-dataset), contains observations from the US used car market and was subset to isolate data by `body_type == Pickup Truck`. It was further tailored to highlight relevant variables that provide insights toward predicting our response variable, `price`.

The impetus for our carrying out this analysis was COVID-19. During the pandemic, with unprecedented migration between urban and rural centers, the prices of used vehicles experienced a notable surge. Additionally, the inventory of said vehicles declined in the US, up to 20% year-over-year in some regions. This piqued our interest as a great opportunity to explore significant indicators influencing used pickup truck prices throughout the states.

The prepared dataset is comprised of the following variables:

|Name   |Type   |Description   |
|:------|:------|:-------------|
|`back_legroom` |`num` |Legspace for backseat passengers in inches |
|`bed` |`Factor w/ 4 levels` |Type of truck bed (Long, Short, etc.) |
|`bed_length` |`num` |Length of truck bed in inches |
|`cabin` |`Factor w/ 5 levels`|Type of passenger cabin (Crew Cab, Extended Cab, etc.) |
|`city` |`Factor w/ 4060 levels` |City where vehicle is being sold |
|`city_fuel_economy` |`num` |MPG mileage in city |
|`daysonmarket` |`num` |Number of days listed for sale |
|`dealer_zip` |`Factor w/ 6897 levels` | Zip code of dealer |
|`engine_cylinders` |`Factor w/ 4 levels` |Number of engine cylinders (4, 6, 8, etc.) |
|`engine_displacement` |`num` |Engine displacement volume |
|`engine_type` |`Factor w/ 4 levels` |Number of engine cylinders (4, 6, 8, etc.) |
|`exterior_color` |`Factor w/ 10 levels` |Exterior color of vehicle |
|`fleet` |`logi` |- - - |
|`frame_damaged` |`logi` |Indicator for frame damage |
|`franchise_dealer` |`logi` |- - - |
|`front_legroom` |`num` |Legspace for frontseat passengers in inches |
|`fuel_tank_volume` |`num` |Capacity of the fuel tank |
|`fuel_type` |`Factor w/ 5 levels` |Type of fuel the vehicle uses to operate |
|`has_accidents` |`logi` |Indicator for whether or not the vehicle has been in an accident |
|`height` |`num` |Height of vehicle in inches |
|`highway_fuel_economy` |`num` |MPG mileage in highway |
|`horsepower` |`num` |Vehicle horsepower |
|`interior_color` |`Factor w/ 10 levels` |Interior color of vehicle |
|`isCab` |`logi` |- - - |
|`is_new` |`logi` |Indicator for a new car |
|`latitude` |`num` |Geographic latitude |
|`length` |`num` |Length of the vehicle in inches |
|`listed_date` |`chr` |Date vehicle was listed for sale |
|`listing_color` |`Factor w/ 10 levels` |Exterior color of vehicle |
|`longitude` |`num` |Geographic longitude |
|`make_name` |`Factor w/ 17 levels` |Manufacturer name |
|`maximum_seating` |`num` |Maximum number of seats in vehicle |
|`mileage` |`num` |Odometer reading |
|`model_name` |`chr` |Model of vehicle from manufacturer |
|`owner_count` |`num` |Number of previous owners |
|`price` |`num` |Sale price of used vehicle (Response) |
|`salvage` |`logi` |Indicator for vehicle being salvage |
|`savings_amount` |`num` |- - - |
|`seller_rating` |`num` |Consumer rating of vehicle seller |
|`theft_title` |`logi` |- - - |
|`transmission` |`Factor w/ 2 levels`|Automatic or Manual transmission (A or M) |
|`transmission_display` |`chr` |Transmission display within vehicle |
|`wheel_system` |`Factor w/ 5 levels` |Vehicle wheel-system (4WD, 4X2, 2WD, etc.) |
|`wheel_system_display` |`chr` |Vehicle wheel-system display within vehicle |
|`wheelbase` |`num` |Wheelbase width in inches |
|`width` |`num` |Width of vehicle in inches |
|`year` |`num` |Manufactured year of vehicle |
|`power_hp` |`num` |- - - |
|`power_rpm` |`num` |- - - |
|`torque_lb_ft` |`num` |Vehicle torque in foot pounds |
|`torque_rpm` |`num` |Vehicle torque RPM |
|`region` |`Factor w/ 9 levels` |US region of vehicle seller |
|`state` |`Factor w/ 49 levels` |State of vehicle seller |

In creating a linear regression model around these variables, we set out to (not only) identify the significant predictors of price for used pickup trucks, but to seek, and potentially offer, strategies for consumers to receive the best value for their money.

Our target metrics, used to define a successful model, are **$RMSE$ <= 6000** &nbsp; and &nbsp; **$R^2$ >= 0.8**.

---

## Methods
### i. Naive Model Analysis

##### Base Model (our hypothesis):
We begin our analysis with an agreed upon "common" model, where we decide the
model predictors to include based on our collective knowledge of the used car
market. As consumers, we focus in on the attributes/variables that we consider
most relevant and most important in determining the value of a vehicle on the
market. This _naive_ base model (our **hypothesis**) serves as our starting position
for exploring model improvements through transformations, interactions, and the
introduction of additional predictor variables.

```{r Naive Base Model (hypothesis), echo=TRUE, results='hide'}
base_model = lm(price ~ engine_displacement + engine_cylinders
                  + highway_fuel_economy + horsepower + maximum_seating
                  + transmission + mileage + year + wheel_system + power_hp
                  + bed_length + cabin + salvage + franchise_dealer + state,
                  data = truck_df)
print_useful_summary_stats(base_model)
```

&nbsp;

##### Full Model (fit all non-character predictors):
Before continuing analysis, we fit an additional _naive_, "full" additive model,
using all numeric and factor predictors without interactions. This approach
improves both $R^2$ and $RMSE$ (when compared to our hypothesis model) due to
the increase of predictors. And so, we conclude that the "full" additive model
provides a better fit to the data. However, looking over the model predictors,
we can see (and validate some of our initial assumptions) that a number of them
have high levels of colinearity. In addition, considering the large number of
predictors, we further conclude that this "full" model is relatively complex.

Despite the colinear relationships between the added predictors, the "full" model
still provides helpful insight and a useful foundation from which we can draw
comparisons against future models. This _base_ "full" model will be used in
assessing the impact of predictor/response transformations and in our outlier
analysis.

```{r Naive Full Model, echo=TRUE, results='hide'}
# create formula for a simple additive model using all non-character variables
full_model_formula = formula(paste("price ~", paste("", no_char_columns, sep = " ", collapse = " +")))

# fit full model
full_model = lm(full_model_formula, data = truck_df)
print_useful_summary_stats(full_model)
```

&nbsp;

### ii. Colinearity & Outlier Analysis
With our dataset cleaned and structured, we seek to better understand the
distributions and correlations found across its variables. The following analysis
uses histograms to visualize these variable distributions and help to identify
the attributes we expect, or assume, will be significant in our final model.

Applying the `pairs` function (generating a scatterplot matrix) and exploring
covariance functions, we break down the relationships between predictors, as well
as how they relate to the response. Identifying the linear relationships, which
indicate colinearity, allows for a first round of model optimization/simplification
by removing redundant (colinear) predictors. The scatterplot matrix further
provides a high-level view of which predictors have a linear, quadratic, or
logarithmic relationship with the response variable.

After analyzing colinearity and variable distribution we focus in on the observations,
themselves. The project's dataset was scraped from the [Cargurus](https://www.cargurus.com/) website
with user-generated input and was not filtered or normalized. Upon further analysis,
we discovered rows containing user errors, e.g - a price of \$10,000 entered as
\$100,000, or a 4 cylinder vehicle mistakenly listed as 6 cylinders. Since we
cannot manually search the data for these erroneous entries but can use outlier
analysis and standardized residuals to draw reasonable assumptions, we can find
and highlight rows that fall within many standard deviations from the population
mean.

A slight simplification to our _naive_ "full" additive model is used to perform
our outlier analysis. This starting model uses all non-character predictors and
has a high $R^2$ with a favorable $RMSE$. Here, we utilize the `rstandard` and
`cooks.distance` functions to draw conclusions and drive future decisions for
optimized model selection.

```{r Histogram Plot Analysis, echo=FALSE}
par(mfrow=c(3,2))
hist(truck_df$year, breaks=length(unique(truck_df$year)),
     xlab="Year", ylab = "Frequency", main = "Histogram of Model Year")
hist(as.integer(truck_df$state), breaks=length(unique(truck_df$state)),
     xlab="State", ylab = "Frequency", main = "Histogram of State")
hist(truck_df$horsepower, breaks=30,
     xlab="Horsepower", ylab = "Frequency", main = "Histogram of Horsepower")
hist(log10(truck_df$mileage), breaks=30,
     xlab="log10(Mileage)", ylab = "Frequency", main = "Histogram of log10(Mileage)")
hist(as.integer(truck_df$maximum_seating), breaks=30,
     xlab="Maximum Seating", ylab = "Frequency", main = "Histogram of Max seating")
```

&nbsp;

```{r Colinearity Analysis pt. 1, echo=FALSE}
pairs_columns = c("price", "engine_cylinders", "engine_displacement", "mileage", "daysonmarket", "year")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns]
pairs(pairs_df, col = "dodgerblue", main = "Scatterplot Matrix (1)")
```

From **Scatterplot Matrix (1)**:

- `price` vs `mileage` appears as if it may benefit if modeled by `1/x`
  - `log(x)` would also work, considering the orders of magnitude
- `engine_cylinders` vs `engine_displacement` are clearly collinear
  - Our model will include `engine_displacement` over `engine_cylinders`
  - This decision is driven by our preference for a `num` variable over a `factor`
- `year` is nearly perfectly linear plus quadratic (x + x^2) with our response, `price`
- `daysonmarket` is, to our surprise, not useful and adds lots of noise

&nbsp;

```{r Colinearity Analysis pt. 2, echo=FALSE}
pairs_columns2 = c("price", "wheel_system", "power_hp", "torque_lb_ft", "maximum_seating", "owner_count")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns2]
pairs(pairs_df, col = "dodgerblue", main = "Scatterplot Matrix (2)")
```

From **Scatterplot Matrix (2)**:

- `power_hp` vs `torque_lb_ft` appear colinear
- `maximum_seating` displays a strong linear relationship with `price`
- `wheel_system` also shows a strong linear relationship with `price`

&nbsp;

```{r Colinearity Analysis pt. 3, echo=FALSE}
pairs_columns3 = c("price", "latitude", "longitude")
pairs_df = random_rows_from_df(truck_df, 0.10)[, pairs_columns2]
pairs(pairs_df, col = "dodgerblue", main = "Scatterplot Matrix (3)")
```

From **Scatterplot Matrix (3)**:

- `latitude` and `longitude` have linear relationships to `price`
  - This relationship appears to have no slope (horizontal)
- The variance in `price` vs `latitude` OR `longitude` is constant
- `latitude` and `longitude` do not appear to be useful predictors of the response
- The plot of `latitude` vs `longitude` shows a map of the United States 

&nbsp;

##### Variance Inflation Factors
As a compliment to our `pairs` scatterplot matrices, we examine variance inflation
factors (`vif`) to determine which predictors are colinear with 1 or more of the
others. The `vif` measures the proportion of the variance within a single predictor
that can be explained by a linear combination of the other predictors. A high `vif`
(>5) indicates colinearity.

```{r Colinearity Analysis pt. 4, echo=FALSE, warning=FALSE}
v = vif(full_model)
v[v > 5]
```

**Key Observations**:

- `exterior_color` & `interior_color` look to be poor predictors
  - multiple `exterior_color` values have a high `vif`

- `cylinders`, `engine_type`, `engine_displacement`, `*_fuel_economy`, `horsepower`,
`power_rpm` and `torque_lb_ft` all have a high `vif`
-  This supports some of our initial assumptions made, when drawing our hypothesis

- `height`, `length`, and `owner_count` each have particularly high `vif`
  - We'd like to better understand with which predictors these are colinear

- Multiple `regions` and `states` have a high `vif`
  - This will become significant, as we seek to build a global prediction model

```{r Colinearity Analysis pt. 5, echo=TRUE, message=FALSE, warning=FALSE}
full_model_no_region_formula = update.formula(
  full_model_formula, ~ . -region -latitude -longitude)
fm1 = lm(full_model_no_region_formula, data = truck_df)
v = vif(fm1)
v[v > 5]

# fit with removed state predictor
full_model_no_state_formula = update.formula(full_model_formula, ~ . -state -latitude -longitude)
fm2 = lm(full_model_no_state_formula, data = truck_df)
v = vif(fm2)
v[v > 5]
```

The `state` predictor continues to return a high `vif` even with `region` removed
from the model. It is worth noting that the factor values of `state` with high
`vif` have changed. TX and NY now return high `vif` values, while AR, CO, NE, NH
no longer do.

We found it safe to conclude that `state` is colinear with `region`, and that
individual factors within the `state` variable are colinear with each other.

Removing the `state` predictor results in `region` having a low `vif`. This outcome
further supports our conclusions on their colinearity.

```{r Colinearity Analysis pt. 6, include=FALSE}
free_memory(pairs_df)
```

##### Outlier Removal
Next, we attempt to identify the outliers in our data (observations which do not
fit the model well). In our analysis, we look at these observations to measure their
respective leverages and influence on our model's predictors. We expect that these
observations will have large residuals.

```{r Outlier Analysis pt. 1, echo=TRUE}
# create formula for simplified additive model omitting character data types
columns_of_interest = setdiff(no_char_columns, c("latitude", "longitude"))
no_char_additive_formula = formula(paste("price ~", paste("", columns_of_interest, sep = " ", collapse = " +")))
outlier_fit_add = lm(no_char_additive_formula, data = truck_df)
print_useful_summary_stats(outlier_fit_add)

count_gt_stddev = c(length(which(rstandard(outlier_fit_add) > 1)),
  length(which(rstandard(outlier_fit_add) > 2)),
  length(which(rstandard(outlier_fit_add) > 3)),
  length(which(rstandard(outlier_fit_add) > 4)))
number_of_stddevs = c(1,2,3,4)
model_results = data.frame(number_of_stddevs, count_gt_stddev)
colnames(model_results) = c("# of stddev", "Count of rows > stddev")
knitr::kable(model_results)

# cooks.distance tests (intersection of high-outlier, high-influence is 95%)
# outliers flagged for removal
high_influence_rows = which(cooks.distance(outlier_fit_add) > 4 / length(outlier_fit_add$residuals))
length(high_influence_rows)
intersection_of_high_inf_high_outlier = intersect(
  which(rstandard(outlier_fit_add) > 3), high_influence_rows)
(pct = length(intersection_of_high_inf_high_outlier) / count_gt_stddev[3] * 100)
intersection_of_high_inf_high_outlier = intersect(
  which(rstandard(outlier_fit_add) > 4), high_influence_rows)
(pct = length(intersection_of_high_inf_high_outlier) / count_gt_stddev[4] * 100)

# analyze outlier rows to check for commonality
outlier_rows_to_remove = intersection_of_high_inf_high_outlier 
outlier_df = truck_df[outlier_rows_to_remove,]
```

- **`r count_gt_stddev[3]`** rows **> 3** standard deviations from the mean
- **`r count_gt_stddev[4]`** rows **> 4** standard deviations from the mean

A significant percentage (``r pct`%`) of observations with large-residuals also
have high leverage and, therefore, have high influence. These observations are
good candidates for exclusion from our dataset.

```{r Outlier Analysis pt. 2, echo=FALSE}
par(mfrow=c(2,2))

hist(outlier_df$year, breaks=length(unique(outlier_df$year)),
     xlab="Year", ylab = "Frequency", main = "Histogram of Model Year Outliers")
hist(as.integer(outlier_df$state), breaks=length(unique(outlier_df$state)),
     xlab="State", ylab = "Frequency", main = "Histogram of State Outliers")
hist(log10(outlier_df$mileage), breaks=30,
     xlab="log10(Mileage)", ylab = "Frequency", main = "Histogram of log10(Mileage) Outliers")
```

We continue our exploration by refitting the "full" model with outliers removed
based on their being influential (measured by `rstandard` and `cooks.distance`).
Reviewing the affect of the outlier removal on the model's coefficients, helps us
confirm whether or not there is significant influence from these observations.

```{r Outlier Analysis pt. 3, echo=TRUE}
# refitted full model with outliers removed
no_outlier_fit_add = lm(no_char_additive_formula, data = truck_df[-outlier_rows_to_remove,])
# summary(no_outlier_fit_add)
print_useful_summary_stats(no_outlier_fit_add)

# percentage change in model coefficients
pct_chng_in_model_coeff = abs(
  (coef(no_outlier_fit_add) - coef(outlier_fit_add))
  / coef(no_outlier_fit_add)) * 100

# check for coefficients with a > 10% change
pct_chng_in_model_coeff[pct_chng_in_model_coeff > 10.0]

# compare removed outlier models
plot_fitted_resid(outlier_fit_add)
plot_fitted_resid(no_outlier_fit_add)
```

From the above code chunk, we find that the $RMSE$ and $R^2$ values of the model
are, mostly, unaffected by the removal of outlier observations. There is, however,
a large percentage change (~50 - 400%) in some of the model's coefficients ($\beta$)
after refitting. We conclude that this change is rather significant, considering
there are only ``r count_gt_stddev[4]` outlier rows > 4 stddev`
and ``r nrow(truck_df)`` in the dataset.

We now continue our analysis with the influential outliers removed from our
dataset (assuming they are user error, and not relevant to our model).


```{r remove_outliers_from_truck_df, echo=TRUE}
truck_df = truck_df[-outlier_rows_to_remove,]
```

&nbsp;

```{r Outlier Analysis pt. 4, include=FALSE}
free_memory(no_outlier_fit_add)
free_memory(outlier_fit_add)
```

&nbsp;

### iii. Regional Model Analysis - Mid Atlantic
As part of our approach to fitting a useful linear model, we decided to parse our
dataset by `region`. In doing so, we allowed ourselves the opportunity to not
only divide and conquer, but to further determine the efficacy of our analysis
by comparing our findings across the regions. This section focuses on our model
exploration of the **Mid-Atlantic** (NY, PA, and NJ), but additional analysis
can be found in the [Appendix](#appendix).

```{r Regional Model Analysis - Mid Atlantic, echo=TRUE}
set.seed(12345)
par(mfrow=c(1,1))

# remove unusable character variables from dataframe
cols_to_keep = append(intersect(no_char_columns, colnames(truck_df)), "price")
truck_df = truck_df[, cols_to_keep]

# test/train split of dataset
trn_idx = sample(nrow(truck_df), nrow(truck_df) * 0.8)
truck_df_trn = truck_df[trn_idx,]
truck_df_tst = truck_df[-trn_idx,]

# process dataset for mid atlantic region analysis
mid_atlantic_data = truck_df_trn[truck_df_trn$region == "Mid Atlantic",]
drops = c("region", "state")
mid_atlantic_data = mid_atlantic_data[ , !(names(mid_atlantic_data) %in% drops)]

# fit mid_atlantic_base_model
base_predictors = c("engine_displacement", "engine_cylinders", "highway_fuel_economy", "horsepower", "maximum_seating", "transmission", "mileage", "year", "wheel_system", "power_hp", "bed_length", "cabin", "salvage", "franchise_dealer" )
base_predictors_formula = formula(paste("price ~", paste("", base_predictors, sep = " ", collapse = " +")))
mid_atlantic_base_model = lm(base_predictors_formula, data = mid_atlantic_data)
print_useful_summary_stats(mid_atlantic_base_model)

# fit mid_atlantic_full_model
mid_atlantic_full_model = lm(price ~ ., data = mid_atlantic_data)
print_useful_summary_stats(mid_atlantic_full_model)

# fit optimized model via backward BIC search
# BIC chosen over AIC in order to favor less predictors in model
nb = length(resid(mid_atlantic_base_model))
base_model_bic = step(mid_atlantic_base_model, direction = "backward", k = log(nb), trace = 0)
summary(base_model_bic)
print_useful_summary_stats(base_model_bic)

# add interactions to base model to explore significance
base_predictors = c("engine_displacement",
                    "engine_cylinders",
                    "highway_fuel_economy",
                    "horsepower",
                    "maximum_seating",
                    "transmission",
                    "mileage",
                    "year",
                    "wheel_system",
                    "power_hp",
                    "bed_length",
                    "cabin",
                    "salvage",
                    "franchise_dealer")
base_interactions = combn(base_predictors, 2, FUN = paste, collapse = ":")
base_interactions_predictors = c(base_predictors, base_interactions)
base_predictors_formula = formula(paste("price ~", paste("", base_interactions_predictors, sep = " ", collapse = " +")))
mid_atlantic_int_model = lm(base_predictors_formula, data = mid_atlantic_data)
print_useful_summary_stats(mid_atlantic_int_model)

# refit model with identified significant interactions using the predictors' p-values
significant_int_predictors = c("year:bed_length",
                               "year:mileage",
                               "maximum_seating:bed_length",
                               "engine_cylinders:highway_fuel_economy",
                               "engine_cylinders:horsepower",
                               "engine_displacement:engine_cylinders",
                               "engine_displacement:highway_fuel_economy")
improved_interactions_predictors = c(base_predictors, significant_int_predictors)
base_predictors_formula = formula(paste("price ~", paste("", improved_interactions_predictors, sep = " ", collapse = " +")))
improved_mid_atlantic_int_model = lm(base_predictors_formula, data = mid_atlantic_data)
print_useful_summary_stats(improved_mid_atlantic_int_model)
```

Seen in the above chunk, we concluded that adding two-way interaction terms is
notably significant and makes valubale improvements to our model. After looking
over the predictors' p-values, we refit the base model, including only the most
significant interaction terms in order to maintain the least number of variables.

&nbsp;

### iv. Significant Variable Transformations and Interactions
```{r Significant Variable Transformations and Interactions pt. 1, results='hide'}
boxcox(improved_mid_atlantic_int_model, plotit = TRUE)

# fitted vs residuals and QQ plots for improved_mid_atlantic_int_model
plot_fitted_resid(improved_mid_atlantic_int_model)
plot_qq(improved_mid_atlantic_int_model)

# refit log-transformed response model
base_predictors_formula = formula(paste("log(price) ~", paste("", improved_interactions_predictors, sep = " ", collapse = " +")))
log_mid_atlantic_int_model = lm(base_predictors_formula, data = mid_atlantic_data)
print_useful_summary_stats(log_mid_atlantic_int_model)
# fitted vs residuals plot for log_mid_atlantic_int_model
plot_fitted_resid(log_mid_atlantic_int_model)
```

From the Box-Cox analysis, log-likelihood reaches a maximum when $\lambda$ is
between `0` and `0.5`. Therefore, we choose to have a log transformation on our
response variable, `price`.

Comparing the **fitted vs residuals** plots from before and after the applied
log-transformation of the response variable, along with our analysis of the
results from the log model, we see very significant improvements.

At this point, we have arrived on a well-fitted `price` prediction regional model
for the used truck dataset. For us to achieve an equally effective global model
for all US data, we require further ANOVA testing. This exploration will determine
the significance of including `region` as a model predictor.

```{r Significant Variable Transformations and Interactions pt. 2, warning=FALSE}
# use regional model to fit a global model to all us data (not including region as a predictor)
base_predictors_formula = formula(paste("log(price) ~", paste("", improved_interactions_predictors, sep = " ", collapse = " +")))
global_model = lm(base_predictors_formula, data = truck_df_trn)
print_useful_summary_stats(global_model)

# ANOVA test global model with region vs without region as a predictor
add_region = c(improved_interactions_predictors, "region")
formula = formula(paste("log(price) ~", paste("", add_region, sep = " ", collapse = " +")))
global_model_add_region = lm(formula, data = truck_df_trn)
print_useful_summary_stats(global_model_add_region)
anova(global_model, global_model_add_region)["Pr(>F)"]

# diagnostics for global model
rmse_trn = sqrt(mean((truck_df_trn$price - predict(global_model, truck_df_trn)) ^ 2))
rmse_tst = sqrt(mean((truck_df_tst$price - predict(global_model, truck_df_tst)) ^ 2))

# diagnostics for global model with region
(rmse_trn = sqrt(mean((truck_df_trn$price - predict(global_model_add_region, truck_df_trn)) ^ 2)))
(rmse_tst = sqrt(mean((truck_df_tst$price - predict(global_model_add_region, truck_df_tst)) ^ 2)))
```

Per the results of the ANOVA, it is clear that `region` is significant as a
predictor of `price`. Therefore, we would like to keep the predictor in our model.
Considering the usefulness of `region`, however, we are also quite confident
that we have constructed a well-fitting global model and can make sound decisions
around its results.

---

## Results
Through the collective effort made throughout the [Methods](#methods) section, we
discovered that there were no significant differences between the fitted models
when parsed by `region`. From these findings, we selected a best-fitting base
model with common predictors that were significant across all regions. Using this
base model, along with the initial "full" additive model, we applied the
demonstrated analysis and selection techniques to finalize a region-agnostic
(global) fit for predicting the response variable, `price`.

&nbsp;

##### Model Comparison
|Model                          |$(p - 1)$                                             |RMSE                                                 |$R^2$                                                      |BP Test                                                                                            |Shapiro-Wilk Test|
|:------------------------------|:-----------------------------------------------------|:----------------------------------------------------|:----------------------------------------------------------|:--------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------|
|**Base Model**                 |`r length(coef(mid_atlantic_base_model)) - 1`         |`r cal_rmse(mid_atlantic_base_model)`                |`r summary(mid_atlantic_base_model)$r.squared`             |`r ifelse(bptest(mid_atlantic_base_model)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$        |`r ifelse(shapiro.test(sample(resid(mid_atlantic_base_model), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$        |
|**Full Model**                 |`r length(coef(mid_atlantic_full_model)) - 1`         |`r cal_rmse(mid_atlantic_full_model)`                |`r summary(mid_atlantic_full_model)$r.squared`             |`r ifelse(bptest(mid_atlantic_full_model)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$        |`r ifelse(shapiro.test(sample(resid(mid_atlantic_full_model), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$        |
|**Base BIC Model**             |`r length(coef(base_model_bic)) - 1`                  |`r cal_rmse(base_model_bic)`                         |`r summary(base_model_bic)$r.squared`                      |`r ifelse(bptest(base_model_bic)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$                 |`r ifelse(shapiro.test(sample(resid(base_model_bic), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$                 |
|**Base Interaction Model**     |`r length(coef(mid_atlantic_int_model)) - 1`          |`r cal_rmse(mid_atlantic_int_model)`                 |`r summary(mid_atlantic_int_model)$r.squared`              |`r ifelse(bptest(mid_atlantic_int_model)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$         |`r ifelse(shapiro.test(sample(resid(mid_atlantic_int_model), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$         |
|**Improved Interaction Model** |`r length(coef(improved_mid_atlantic_int_model)) - 1` |`r cal_rmse(improved_mid_atlantic_int_model)`        |`r summary(improved_mid_atlantic_int_model)$r.squared`     |`r ifelse(bptest(improved_mid_atlantic_int_model)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$|`r ifelse(shapiro.test(sample(resid(improved_mid_atlantic_int_model), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$|
|**Log Transform Model**        |`r length(coef(log_mid_atlantic_int_model)) - 1`      |`r cal_rmse(log_mid_atlantic_int_model)`             |`r summary(log_mid_atlantic_int_model)$r.squared`          |`r ifelse(bptest(log_mid_atlantic_int_model)$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$     |`r ifelse(shapiro.test(sample(resid(log_mid_atlantic_int_model), 5000))$p.value > 0.1, 'Fail to reject', 'Reject')` $H_0$     |

&nbsp;

From the above table, it quickly becomes clear why we have chosen the `log`
response model as our final prediction model.

Starting with the base model, a $R^2$ of 0.7536 is among the lowest that we found
across all of our linear models. This comes as no surprise, however, given we
started with a simple and "naive" approach.

Similarly unsurprising, when analyzing the full model, we came across a much higher
$R^2$ and a favorable RMSE. Unfortunately, as expected with full, exhaustive models,
the number of predictors has become quite large, making the full model complex
and not an ideal model for interpretation.

After applying a backward search, leveraging BIC to penalize against keeping
predictors from our base model, we find ourselves with a model that is the easiest
to interpret. Unfortunately, as much as we seek to simplify our models in linear
regression, we are focusing on the strength of our models as predictors. Our
backwards BIC selected model sacrifices prediction power, as seen by the $R^2$
and RMSE decreasing and increasing respectively. In the end, we're left with
results that are not far off from our initial "naive" base model.

As a result of our findings across the base, full, and backwards BIC models, we
decided to introduce model interactions. In our base interaction model, we make
use of all significant predictors along with all two-way interactions between them.
In doing so, we found some improvements to the $R^2$ and RMSE. In fact, they wound
up very close to the full additive model. Alas, as we found with the full additive
model, the number of predictors becomes painfully extraordinarily large relative
to the other models, making the base interaction model both complex and difficult
to interpret.

Before arriving at our `log` response model, we sought to improve the base
interaction model by highlighting its most significant interactions. This
approach resulted in a significant decrease in the number of predictors at
40 and a decent return in $R^2$ and RMSE. The improved interaction model boasts
being relatively less complex and easier to interpret, when compared to its
predecessor. And, although it provides results that may be adequate, we ultimately
decided to explore the transformation of the response as a final model improvement.

&nbsp;

```{r Prediction Setup, include=FALSE}
num_size = 2000
drops = c("region")
tst_idx = sample(nrow(truck_df_tst), num_size)
tst_data = truck_df_tst[tst_idx,]
tst_data = tst_data[, cols_to_keep]
tst_data = tst_data[ , !(names(tst_data) %in% drops)]

## base model prediction
prd_base_err = predict(mid_atlantic_base_model, tst_data) - tst_data$price
prd_base_err_mean = mean(prd_base_err)
prd_base_err_std = sd(prd_base_err)

## full model prediction
prd_full_err = predict(mid_atlantic_full_model, tst_data) - tst_data$price
prd_full_err_mean = mean(prd_full_err)
prd_full_err_std = sd(prd_full_err)

## BIC model prediction
prd_bic_err = predict(base_model_bic, tst_data) - tst_data$price
prd_bic_err_mean = mean(prd_bic_err)
prd_bic_err_std = sd(prd_bic_err)

## interaction model prediction
prd_bicint_err = predict(mid_atlantic_int_model, tst_data) - tst_data$price
prd_bicint_err_mean = mean(prd_bicint_err)
prd_bicint_err_std = sd(prd_bicint_err)

## improved interaction model prediction
prd_imp_bicint_err = predict(improved_mid_atlantic_int_model, tst_data) - tst_data$price
prd_imp_bicint_err_mean = mean(prd_imp_bicint_err)
prd_imp_bicint_err_std = sd(prd_imp_bicint_err)

## log transformation model prediction
prd_log_err = exp(predict(log_mid_atlantic_int_model, tst_data)) - tst_data$price
prd_log_err_mean = mean(prd_log_err)
prd_log_err_std = sd(prd_log_err)
```

##### Model Prediction
|Model                          |Mean Prediction Error                                 |Prediction Error Standard Deviation |
|:------------------------------|:---------------------------------------------------:|:-----------------------------------:|
|**Base Model**                 |`r prd_base_err_mean`                                 |`r prd_base_err_std`                |
|**Full Model**                 |`r prd_full_err_mean`                                 |`r prd_full_err_std`                |
|**Base BIC Model**             |`r prd_bic_err_mean`                                  |`r prd_bic_err_std`                 |
|**Base Interaction Model**     |`r prd_bicint_err_mean`                               |`r prd_bicint_err_std`              |
|**Improved Interaction Model** |`r prd_imp_bicint_err_mean`                           |`r prd_imp_bicint_err_std`          |
|**Log Transform Model**        |`r prd_log_err_mean`                                  |`r prd_log_err_std`                 |

&nbsp;

##### Base Model
```{r Base Model (hypothesis)}
format(names(coef(mid_atlantic_base_model)[-1]), fill = getOption("width"))
hist(prd_base_err, breaks = 100, main = "Residual Distribution of Base model")
```

&nbsp;

##### Full Model
```{r Full Model}
format(names(coef(mid_atlantic_full_model)[-1]), fill = getOption("width"))
hist(prd_full_err, breaks = 100, main = "Residual Distribution of Full Model")
```

&nbsp;

##### Improved Interaction Model
```{r Improved Interaction Model}
format(names(coef(improved_mid_atlantic_int_model)[-1]), fill = getOption("width"))
hist(prd_imp_bicint_err, breaks = 100, main = "Residual Distribution of Improved Interaction Model")
```

&nbsp;

##### Finalized Global Model (Log Transform)
```{r Finalized Global Model (Log Transform)}
format(names(coef(log_mid_atlantic_int_model)[-1]), fill = getOption("width"))
hist(prd_log_err, breaks = 100, main = "Residual Distribution of Log Response Model")
```

&nbsp;

##### Fitted vs Residual & QQ Plots
```{r Model FvR Plots}
par(mfrow=c(2,2))

plot_fitted_resid(mid_atlantic_base_model, main="Base Model Fitted vs. Residuals")
plot_fitted_resid(mid_atlantic_full_model, main="Full Model Fitted vs. Residuals")
plot_fitted_resid(improved_mid_atlantic_int_model, main="Improved Interaction Model Fitted vs. Residuals")
plot_fitted_resid(log_mid_atlantic_int_model, main="Log Response Model Fitted vs. Residuals")
```

_Base Model (top-left), Full Model (top-right), Improved Interaction Model (bottom-left), Log Response Model (bottom-right)_

&nbsp;

```{r Model QQ Plots}
par(mfrow=c(2,2))

plot_qq(mid_atlantic_base_model, main="Base Model Q-Q Plot")
plot_qq(mid_atlantic_full_model, main="Full Model Q-Q Plot")
plot_qq(improved_mid_atlantic_int_model, main="Improved Interaction Model Q-Q Plot")
plot_qq(log_mid_atlantic_int_model, main="Log Response Model Q-Q Plot")
```

_Base Model (top-left), Full Model (top-right), Improved Interaction Model (bottom-left), Log Response Model (bottom-right)_

---

## Discussion
As previously demonstrated in the [Results](#results) section, the diagnostic
analysis of each of our models failed to support the **LINE** assumptions of
linear regression. More specifically, the assumptions of error (residuals)
normality and constant variance.

The null hypotheses, for each of the Breusch-Pagan tests we conducted for constant
variance, were all rejected. The same is true for each Shapiro-Wilk test performed,
thereby indicating that the "normal distribution" of our models' residuals is
suspect. As noted in our class lectures, these tests are not 100% accurate, but
definitely raise questions about our dataset, suggesting that it may not be
"ideal" (or in an optimal state) per **LINE** criteria. Nonetheless, we choose to
not write off our efforts, as our model may be "close enough" to be statistically
useful.

Reviewing the **fitted vs residuals** plots during our analysis, revealed that
data, when parsed by region, has a mean of nearly `0` at all fitted value data
points for our `Improved Interaction Model`. The spread of the fitted values is
higher at lower prices than at higher prices, though not by much. We can, therefore,
conclude that our per-region data is linear, and reflects close to constant variance,
albeit lower at higher `price` points.

Examining the **Q-Q** plots for our models shows that, for by-region analysis, the
distribution of their residuals follows the normal **Q-Q** line, even at extreme
values of `price`. This information supports our conclusion that, when fit by
region, our models' residuals appear to be distributed normally. Conversely, when
the same models are used to fit global (all-regions) data the **Q-Q** plot shows
notable deviations at high and low theoretical values of `price`.

The combined evidence of the **BP** and **Shapiro-Wilk** tests, along with the
**fitted vs residuals** and **Q-Q** plots, indicate that our regression analysis
reflects a proper adherence to the **LINE** assumption criteria at the regional
model level. When the same models are applied to observations spanning the entire
US, we begin to lose confidence in our assumptions, as error normality decreases
and model variance increases.

Had we had the time, we believe some additional effort could have been made to
refine the accuracy of the model(s) on the full US dataset. It would seem
reasonable to assume that using a larger dataset would likely expose our model(s)
to larger sets of outlier observations with potentially unusual or invalid data.
Additionally, we did point out that `region` and `state` were both identified as
significant predictors from the start of our analysis. Predictors that have
proven to be difficult to model. `state` and `region` have a high probability of
being colinear, but may not necessarily be colinear, depending on the region(s)
and state(s) in question.

Some further analysis regarding which states are truly/particularly significant
and which are colinear may have allowed us the ability to better process our
dataset. One idea we had considered was clustering states into more granular
regions, which may have potentially improved our RMSE, error normality, and
constant variance assumptions.

We also considered linking our geographic information, (e.g. - `latitude`, `longitude`,
`zip_code`, `state`, etc.) with other datasets (e.g. - demographic, financial) to
aid in uncovering some useful interactions between variables such as `income`, `age`,
or `climate`. We believe that such an approach would have also led to improvements
of our models' performance. That said, we chose not to overburden ourselves with
the "nice-to-haves", given our time constraint. In the end, however, we suspect,
due to the differences we observed between our regional and global models, that
a greater focus on geography could have helped to improve our prediction results.

---

## Appendix
### i. Data Cleaning
Using the following code, the dataset downloaded from [Kaggle](https://www.kaggle.com/ananaymital/us-used-cars-dataset) is imported and processed through a series of functions which normalize fields, remove useless variables, and set the data structures within the dataset (numeric, factor, etc.).
```{r Data Cleaning, eval=FALSE}
# fix colors function
fix_colors = function(dataset) {
  colors = c('gray', 'black', 'white', 'green', 'blue', 'red', 'silver', 'yellow', 'brown')
  
  for (i in 1:length(colors)) {
    dataset[grep(colors[i], dataset, fixed = TRUE)] = colors[i]
  }
  
  dataset = ifelse(dataset %in% colors, dataset, 'other')
  toupper(dataset)
}

clean_dataset = function() {
  # fix empty and NA entries
  dataset$back_legroom = ifelse(is.na(dataset$back_legroom), 0, dataset$back_legroom)
  dataset$bed = ifelse(is.na(dataset$bed), 'Not Listed', dataset$bed)
  dataset$bed_length = ifelse(is.na(dataset$bed_length), 0, dataset$bed_length)
  dataset$cabin = ifelse(is.na(dataset$cabin), 'Not Listed', dataset$cabin)
  dataset$fleet = ifelse(is.na(dataset$fleet), FALSE, dataset$fleet)
  dataset$frame_damaged = ifelse(is.na(dataset$frame_damaged), FALSE, dataset$frame_damaged)
  dataset$front_legroom = ifelse(is.na(dataset$front_legroom), 0, dataset$front_legroom)
  dataset$fuel_tank_volume = ifelse(is.na(dataset$fuel_tank_volume), 0, dataset$fuel_tank_volume)
  dataset$isCab = ifelse(is.na(dataset$isCab), FALSE, dataset$isCab)
  dataset$has_accidents = ifelse(is.na(dataset$has_accidents), FALSE, dataset$has_accidents)
  dataset$height = ifelse(is.na(dataset$height), 0, dataset$height)
  dataset$highway_fuel_economy = ifelse(is.na(dataset$highway_fuel_economy), 0, dataset$highway_fuel_economy)
  dataset$horsepower = ifelse(is.na(dataset$horsepower), 0, dataset$horsepower)
  dataset$isCab = ifelse(is.na(dataset$isCab), FALSE, dataset$isCab)
  dataset$is_new = ifelse(is.na(dataset$is_new), FALSE, dataset$is_new)
  dataset$length = ifelse(is.na(dataset$length), 0, dataset$length)
  dataset$maximum_seating = ifelse(is.na(dataset$maximum_seating), 0, dataset$maximum_seating)
  dataset$mileage = ifelse(is.na(dataset$mileage), 0, dataset$mileage)
  dataset$owner_count = ifelse(is.na(dataset$owner_count), 0, dataset$owner_count)
  dataset$salvage = ifelse(is.na(dataset$salvage), FALSE, dataset$salvage)
  dataset$seller_rating = ifelse(is.na(dataset$seller_rating), 0, dataset$seller_rating)
  dataset$theft_title = ifelse(is.na(dataset$theft_title), FALSE, dataset$theft_title)
  dataset$transmission = ifelse(dataset$transmission == 'CVT' | dataset$transmission == 'Dual Clutch', 'A', dataset$transmission)
  dataset$wheelbase = ifelse(is.na(dataset$wheelbase), 0, dataset$wheelbase)
  dataset$width = ifelse(is.na(dataset$width), 0, dataset$width)
  
  # normalize colors
  dataset$exterior_color = tolower(dataset$exterior_color)
  dataset$exterior_color = fix_colors(dataset$exterior_color)
  dataset$interior_color = tolower(dataset$interior_color)
  dataset$interior_color = fix_colors(dataset$interior_color)
  dataset$listing_color = tolower(dataset$listing_color)
  dataset$listing_color = fix_colors(dataset$listing_color)
  
  # remove any remaining NA entries
  dataset = subset(dataset, !is.na(back_legroom))
  dataset = subset(dataset, back_legroom != '--')
  dataset = subset(dataset, !is.na(bed))
  dataset = subset(dataset, bed_length != '--')
  dataset = subset(dataset, !is.na(cabin))
  dataset = subset(dataset, !is.na(city_fuel_economy))
  dataset = subset(dataset, !is.na(engine_cylinders))
  dataset = subset(dataset, !is.na(engine_type))
  dataset = subset(dataset, !is.na(fuel_type))
  dataset = subset(dataset, !is.na(power))
  dataset = subset(dataset, !is.na(torque))
  dataset = subset(dataset, !is.na(transmission))
  
  # clean up character data (optimized)
  reg_num = "[:digit:]+.?[:digit:]?"
  reg_rpm = "[:digit:],[:digit:]+"
  
  dataset$back_legroom = str_extract(dataset$back_legroom, reg_num)
  dataset$bed_length = str_extract(dataset$bed_length, reg_num)
  dataset$front_legroom = str_extract(dataset$front_legroom, reg_num)
  dataset$fuel_tank_volume = str_extract(dataset$fuel_tank_volume, reg_num)
  dataset$engine_cylinders = str_extract(dataset$engine_cylinders, reg_num)
  dataset$engine_type = str_extract(dataset$engine_type, reg_num)
  dataset$height = str_extract(dataset$height, reg_num)
  dataset$length = str_extract(dataset$length, reg_num)
  dataset$maximum_seating = str_extract(dataset$maximum_seating, reg_num)
  dataset$wheelbase = str_extract(dataset$wheelbase, reg_num)
  dataset$width = str_extract(dataset$width, reg_num)
  dataset$power_hp = str_extract(dataset$power, reg_num)
  dataset$power_rpm = str_extract(dataset$power, reg_rpm)
  dataset$torque_lb_ft = str_extract(dataset$torque, reg_num)
  dataset$torque_rpm = str_extract(dataset$torque, reg_rpm)

  # remove unnecessary variables
    dataset = subset(dataset,
                            select = -c(vin,
                                        bed_height,
                                        body_type,
                                        combine_fuel_economy,
                                        description,
                                        franchise_make,
                                        is_certified,
                                        is_cpo,
                                        is_oemcpo,
                                        listing_id,
                                        main_picture_url,
                                        major_options,
                                        vehicle_damage_category,
                                        power,
                                        torque,
                                        sp_id,
                                        sp_name,
                                        trimId,
                                        trim_name))

  dataset = set_data_structure(dataset)
  dataset
}

set_data_structure = function(dataset) {
  # update variable structure
  dataset$back_legroom = as.numeric(dataset$back_legroom)
  dataset$bed = as.factor(dataset$bed)
  dataset$bed_length = as.numeric(dataset$bed_length)
  dataset$cabin = as.factor(dataset$cabin)
  dataset$city = as.factor(dataset$city)
  dataset$dealer_zip = as.factor(dataset$dealer_zip)
  dataset$engine_cylinders = as.factor(as.numeric(dataset$engine_cylinders))
  dataset$engine_type = as.factor(as.numeric(dataset$engine_type))
  dataset$exterior_color = as.factor(dataset$exterior_color)
  dataset$front_legroom = as.numeric(dataset$front_legroom)
  dataset$fuel_tank_volume = as.numeric(dataset$fuel_tank_volume)
  dataset$fuel_type = as.factor(dataset$fuel_type)
  dataset$height = as.numeric(dataset$height)
  dataset$interior_color = as.factor(dataset$interior_color)
  dataset$length = as.numeric(dataset$length)
  dataset$listing_color = as.factor(dataset$listing_color)
  dataset$make_name = as.factor(dataset$make_name)
  dataset$maximum_seating = as.factor(dataset$maximum_seating)
  dataset$region = as.factor(dataset$region)
  dataset$state = as.factor(dataset$state)
  dataset$transmission = as.factor(dataset$transmission)
  dataset$wheel_system = as.factor(dataset$wheel_system)
  dataset$wheelbase = as.numeric(dataset$wheelbase)
  dataset$width = as.numeric(dataset$width)
  dataset$power_hp = as.numeric(dataset$power_hp)
  dataset$power_rpm = as.numeric(dataset$power_rpm)
  dataset$torque_lb_ft = as.numeric(dataset$torque_lb_ft)
  dataset$torque_rpm = as.numeric(dataset$torque_rpm)
  dataset$year = as.numeric(dataset$year)
  dataset
}

# clean up data
used_truck_data = clean_dataset(read_csv({{ CSV_FILE_PATH }}))
```

&nbsp;

### ii. Utility Functions
```{r Utility Functions, eval=FALSE}
# plot fitted vs residuals
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model),
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

# Q-Q plot
plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

# calculate RMSE
cal_rmse = function(model){
  sqrt(mean(resid(model) ^ 2))
}

# calculate leave one out cross validation
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# delete large object obj, call the garbage collector to trigger garbage
# collector to clean up deleted obj.
free_memory = function(obj = data.frame()) {
  rm(obj)
  gc()
}

# select a % of random rows in the data frame. Rows are chosen using a uniform
# distribution.
random_rows_from_df = function(df = data.frame(), pct = 0.10) {
  rowidx = as.integer(runif(round(nrow(df) * 0.10), min = 0, max = nrow(df)))
  df[rowidx, ]
}

# calculate and display useful model summary statistics
print_useful_summary_stats = function(fit) {
  s = summary(fit)
  d = c(
    resid_std_err = c(s$sigma),
    r.squared = c(s$r.squared),
    loocv_rmse = calc_loocv_rmse(fit),
    rmse = cal_rmse(fit),
    fstatistic = c(s$fstatistic[1]),
    p.value = c(pf(s$fstatistic[1], df1=s$df[1] - 1, df2=s$df[2], lower.tail = FALSE))
  )
  d
}

# display model statistics and diagnostic information
comparison_stats = function(fit) {
  s = summary(fit)
  d = c(
    predictor_numbers = length(fit$coefficients),
    r.squared = c(s$r.squared),
    loocv_rmse = calc_loocv_rmse(fit),
    rmse = cal_rmse(fit)
  )
  d
}

```

&nbsp;

### iii. Identify Regions by Latitude/Longitude
```{r Identify Regions by Latitude/Longitude, eval=FALSE}
# set placeholder value for region variable in data frame
used_truck_data$region = "Other"

# set Region 1 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 37.78076828622018 & used_truck_data$latitude > 37.25035741610745 
                 & used_truck_data$longitude < -121.85307726189703 & used_truck_data$longitude > -122.50488144107447] = "Region 1"

# set Region 2 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 47.78502706081486 & used_truck_data$latitude > 47.12327044705014 
                 & used_truck_data$longitude < -122.04186361166227 & used_truck_data$longitude > -122.61906067660752] = "Region 2"

# set Region 3 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 32.988394538505275 & used_truck_data$latitude > 32.66950915381749 
                 & used_truck_data$longitude < -96.55528632692199 & used_truck_data$longitude > -97.48732384279774] = "Region 3"

# set Region 4 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 30.460499890421573 & used_truck_data$latitude > 29.274940284447997 
                 & used_truck_data$longitude < -97.62876583084027 & used_truck_data$longitude > -98.6657153056594] = "Region 4"

# set Region 5 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 34.14494984518532 & used_truck_data$latitude > 33.68433856779674 
                 & used_truck_data$longitude < -84.19816278737434 & used_truck_data$longitude > -84.7634847112431] = "Region 5"

# set Region 6 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 28.737108891837234 & used_truck_data$latitude > 27.82339991184241
                 & used_truck_data$longitude < -81.22261726061794 & used_truck_data$longitude > -82.66780079513585] = "Region 6"

# set Region 7 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 41.010668887052006 & used_truck_data$latitude > 39.34837978601289 
                 & used_truck_data$longitude < -73.6914252253522 & used_truck_data$longitude > -75.35002087520648] = "Region 7"

# set Region 8 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 42.58129590905933 & used_truck_data$latitude > 42.22364218404111
                 & used_truck_data$longitude < -70.8183880968727 & used_truck_data$longitude > -71.34541809524895] = "Region 8"

# set Region 9 on latitude / longitude conditions
used_truck_data$region[used_truck_data$latitude < 43.30313864345386 & used_truck_data$latitude > 41.5380434345939 
                 & used_truck_data$longitude < -87.20791391134868 & used_truck_data$longitude > -88.34338447422027] = "Region 9"

# set region variable as factor with 9 levels
used_truck_data$region = as.factor(used_truck_data$region)
```

&nbsp;

### iv. Add State & Region Data via `usa` Package
```{r Add State & Region Data via usa Package, eval=FALSE}
# extract US data from 'usa' library and merge with used truck dataset
library(usa)
zcs = usa::zipcodes
zcs = zcs[c("zip", "state")]
used_truck_data = merge(used_truck_data, zcs, by.x = "dealer_zip", by.y = "zip")

# define regions
pacific = c("WA", "OR", "CA", "AK", "HI")  
mountain = c("MT", "ID", "WY", "NV", "UT", "CO", "AZ", "NM")
west_north_central = c("ND", "MN", "SD", "NE", "IA", "KS", "MO")
west_south_central = c("OK", "AR", "TX", "LA")
east_north_central = c("WI", "MI", "IL", "IN", "OH")
east_south_central = c("KY", "TN", "MS", "AL") 
new_england = c("ME", "VT", "NH", "MA","RI","CT")
mid_atlantic = c("NY", "PA", "NJ")
south_atlantic = c("MD","DE", "WV", "VA", "NC", "SC", "GA", "FL", "DC") 

## add regions to dataset
used_truck_data$region = "Other"
used_truck_data[used_truck_data$state %in% pacific,]$region = "Pacific"
used_truck_data[used_truck_data$state %in% mountain,]$region = "Moutain"
used_truck_data[used_truck_data$state %in% west_north_central,]$region = "West North Central"
used_truck_data[used_truck_data$state %in% west_south_central,]$region = "West South Central"
used_truck_data[used_truck_data$state %in% east_north_central,]$region = "East North Central"
used_truck_data[used_truck_data$state %in% east_south_central,]$region = "East South Central"
used_truck_data[used_truck_data$state %in% new_england,]$region = "New England"
used_truck_data[used_truck_data$state %in% mid_atlantic,]$region = "Mid Atlantic"
used_truck_data[used_truck_data$state %in% south_atlantic,]$region = "South Atlantic"
table(used_truck_data$power_rpm)
str(used_truck_data)

# remove unnecessary columns
drops = c("dealer_zip", "city", "listed_date", "model_name", "listed_color")
used_truck_data = used_truck_data[ , !(names(used_truck_data) %in% drops)]

# convert data structure
used_truck_data$power_rpm = as.numeric(gsub(",","",used_truck_data$power_rpm))
used_truck_data$torque_rpm = as.numeric(gsub(",","",used_truck_data$torque_rpm))
used_truck_data = used_truck_data %>% mutate_if(is.character,as.factor)
str(used_truck_data)
```

&nbsp;

### v. Region Analysis Workflow
```{r Region Analysis Workflow, eval=FALSE}
### region analysis steps and model fitting
region_data = used_truck_data[used_truck_data$region == "{{ REGION }}",] # select data for region

# fit naive base model (using predetermined predictors)
base_region_model = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer, data = region_data)
summary(base_region_model) # view naive model summary
calc_loocv_rmse(base_region_model) # calculate leave one out cross validation
cal_rmse(base_region_model) # calculate RMSE
# select naive base model variables
base_data = base_region_model[c("engine_displacement", "engine_cylinders", "highway_fuel_economy", "horsepower", "maximum_seating", "transmission", "mileage", "year", "wheel_system", "power_hp", "bed_length", "cabin", "salvage", "franchise_dealer")]
pairs(base_region_model) # view scatterplot matrix


### explore naive base model with state variable, (state is significant)
drops = c("region") # set field to drop
base_region_model = base_region_model[ , !(names(base_region_model) %in% drops)] # remove drop field(s)
# fit model
base_region_model_state = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer + state, data = region_data)
summary(base_region_model_state) # view naive model summary
calc_loocv_rmse(base_region_model_state) # calculate leave one out cross validation
cal_rmse(base_region_model_state) # calculate RMSE
anova(base_region_model, base_region_model_state) # compare two models for significance


full_region_model = lm(price ~ ., data = region_data) # fit full naive model (using all possible predictors)
calc_loocv_rmse(full_region_model) # calculate leave one out cross validation
cal_rmse(full_region_model) # calculate RMSE


### apply forward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
full_model_bic_forward = step(base_region_model_state, scope = price ~., direction = "forward", k = log(n)) # select model via forward BIC
summary(full_model_bic_forward) # view selected model summary
calc_loocv_rmse(full_model_bic_forward) # calculate leave one out cross validation
cal_rmse(full_model_bic_forward) # calculate RMSE


### apply backward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
region_model_bic = step(full_region_model, direction = "backward", k = log(n),trace = 0) # select model via backward BIC
summary(region_model_bic) # view selected model summary
calc_loocv_rmse(region_model_bic) # calculate leave one out cross validation
cal_rmse(region_model_bic) # calculate RMSE
car::vif(region_model_bic) # view variance inflation factors


### fit improved region model
improved_region_model = lm(formula = price ~ back_legroom  + 
                              engine_cylinders + franchise_dealer + front_legroom + fuel_tank_volume + 
                              fuel_type + height + horsepower + isCab + 
                              is_new  + make_name + maximum_seating + mileage + 
                              seller_rating  + wheel_system + wheelbase + 
                              width + year + power_rpm + torque_lb_ft + torque_rpm + state, data = region_data) # fit model
summary(improved_region_model) # view improved model summary
calc_loocv_rmse(improved_region_model) # calculate leave one out cross validation
cal_rmse(improved_region_model) # calculate RMSE
```

&nbsp;

### vi. Model Analysis for Pacific Region
```{r Model Analysis for Pacific Region, eval=FALSE}
size_num = 5000

cars_pacific = subset(used_truck_data, region == 'Pacific')
cars_idx = sample(nrow(cars_pacific), size_num)
cars_trn = cars_pacific[cars_idx, ]
cars_tst = cars_pacific[-cars_idx, ]

# fit naive base model
fit_common = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy
                  + horsepower + maximum_seating + transmission + mileage
                  + year + wheel_system + power_hp + bed_length + cabin
                  + salvage + franchise_dealer + state, data = cars_trn)
summary(fit_common)

# number of observations
n = length(resid(fit_common))

# fit reduced model using backward BIC search
fit_reduced = step(fit_common, direction = "backward", trace = 0, k = log(n))
summary(fit_reduced)

# calculate RMSE for train / test data
cal_rmse(cars_trn)
cal_rmse(cars_tst)

# compare LOOCV RMSE between common and reduced models
calc_loocv_rmse(fit_common)
calc_loocv_rmse(fit_reduced)

# plot fitted vs residuals
plot(fitted(fit_reduced), resid(fit_reduced), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

# BP test for equal variance
bptest(fit_reduced)

# Shapiro Wilk test for error normality
shapiro.test(resid(fit_reduced))

# plot Box-Cox
boxcox(fit_reduced, plotit = TRUE)

# fit transformed model based on Box-Cox analysis
fit_cox = lm((price ^ 0.26 - 1) / 0.26 ~ engine_displacement + engine_cylinders + 
    highway_fuel_economy + horsepower + maximum_seating + mileage + 
    year + wheel_system + bed_length + franchise_dealer, data = cars_trn)
summary(fit_cox)

# BP test for equal variance
bptest(fit_cox)

# plot fitted vs residuals
plot(fitted(fit_cox), resid(fit_cox), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

Simplified Model after Box-Cox Analysis:
```{r Simplified Model after Box-Cox Analysis, eval=FALSE}
price ~ engine_displacement
        + engine_cylinders
        + highway_fuel_economy
        + horsepower
        + maximum_seating
        + mileage
        + year
        + wheel_system
        + bed_length
        + franchise_dealer
```

- `10` predictors
- $R^2$ = `0.71`

After analyzing Box-Cox, we consider a response transformation of up to a power of 0.3.
This analysis corroborated our decision to apply the log transformation to the response of our selected prediction model.

```{r Additional Model Analysis for Pacific Region, eval=FALSE}
fit_reduced2 = lm(log(price) ~ engine_displacement + engine_cylinders
                  + highway_fuel_economy + horsepower + maximum_seating
                  + mileage + year + wheel_system + bed_length
                  + franchise_dealer,
                  data = cars_trn)
summary(fit_reduced2)
rmse(log(cars_trn$price), predict(fit_reduced2, cars_trn))
rmse(log(cars_tst$price), predict(fit_reduced2, cars_tst))
calc_loocv_rmse(fit_reduced2)
```

- `10` predictors
- $R^2$ = `0.82`
- $RMSE$ = 0.17

```{r Pacific Region Q-Q Plots for Error Normality, eval=FALSE}
qqnorm(resid(fit_reduced2), col = "darkgrey")
qqline(resid(fit_reduced2), col = "dodgerblue", lwd = 2)
```

```{r Additional Model Analysis for Pacific Region (cont.), eval=FALSE}
pairs(log(price) ~ engine_displacement + engine_cylinders + fuel_tank_volume
      + highway_fuel_economy + horsepower + maximum_seating + mileage + year
      + bed_length,
      data = cars_trn)

fit_col = lm(log(price) ~ engine_displacement + engine_cylinders
             + highway_fuel_economy + horsepower + maximum_seating + mileage
             + year + wheel_system + bed_length + franchise_dealer
             + year:bed_length + year:mileage + maximum_seating:bed_length
             + engine_cylinders:highway_fuel_economy + engine_cylinders:horsepower
             + engine_displacement:engine_cylinders
             + engine_displacement:highway_fuel_economy,
             data = cars_trn)
summary(fit_col)
anova(fit_reduced2, fit_col)
rmse(log(cars_trn$price), predict(fit_col, cars_trn))
rmse(log(cars_tst$price), predict(fit_col, cars_tst))
calc_loocv_rmse(fit_col)
```

- $R^2$ = 0.84
- $RMSE$ = 0.16

&nbsp;

### vii. Model Analysis for East South Central Region
```{r Model Analysis for East South Central Region, eval=FALSE}
# load and clean dataset
used_truck_data_clean = read_csv('../Dataset/used_truck_data_with_region.csv')
View(used_truck_data_clean)

used_truck_data = subset(used_truck_data, region == 'East North Central' | region == 'East South Central')
View(used_truck_data)

used_enc = subset(used_truck_data, region == 'East North Central')
used_esc = subset(used_truck_data, region == 'East South Central')

# East South Central
## set structure
used_esc = set_data_structure(used_esc)
used_esc = subset(used_esc, select = -region) # remove single factor variable

# fit model w/o large factor or unnecessary variables (dealer_zip, city, listed_date, model_name, transmission_display, torque_rpm, wheel_system_display)
# (p - 1) = 147
fit_esc_1 = lm(price ~ . - dealer_zip - city - listed_date - model_name - transmission_display - torque_rpm - wheel_system_display, data = used_esc)
summary(fit_esc_1)

most_significant_vars = c(back_legroom + bed_length + cabin + city_fuel_economy + daysonmarket + engine_cylinders + franchise_dealer + front_legroom + fuel_tank_volume + fuel_type + highway_fuel_economy + horsepower + interior_color + isCab + is_new + length + make_name + maximum_seating + mileage + owner_count + salvage + transmission + wheel_system + wheelbase + width + year + power_rpm + state)

# fit model w/ most significant variables from fit_esc_1
# (p - 1) = 111
fit_esc_2 = lm(price ~ back_legroom
               + bed_length
               + cabin
               + city_fuel_economy
               + daysonmarket
               + engine_cylinders
               + franchise_dealer
               + front_legroom
               + fuel_tank_volume
               + fuel_type
               + highway_fuel_economy
               + horsepower
               + interior_color
               + isCab
               + is_new
               + length
               + make_name
               + maximum_seating
               + mileage
               + owner_count
               + salvage
               + transmission
               + wheel_system
               + wheelbase
               + width
               + year
               + power_rpm
               + state, data = used_esc)
summary(fit_esc_2)

fit_2_variables = subset(used_esc, select = c(back_legroom, bed_length, cabin, city_fuel_economy, daysonmarket, engine_cylinders, franchise_dealer, front_legroom, fuel_tank_volume, fuel_type, highway_fuel_economy, horsepower, interior_color, isCab, is_new, length, make_name, maximum_seating, mileage, owner_count, price, salvage, transmission, wheel_system, wheelbase, width, year, power_rpm, state))
pairs(fit_2_variables) # scatterplot matrix

# Select significant model using backward AIC from fit_esc_2
# (p - 1) = 110
fit_esc_3 = step(fit_esc_2, direction = 'backward', trace = 0)
summary(fit_esc_3)

# Select significant model using forward AIC from fit_esc_2
# (p - 1) = 110
mod_start = lm(price ~ 1, data = used_esc)
fit_esc_4 = step(mod_start,
                 scope = price ~ back_legroom
                 + bed_length
                 + cabin
                 + city_fuel_economy
                 + daysonmarket
                 + engine_cylinders
                 + franchise_dealer
                 + front_legroom
                 + fuel_tank_volume
                 + fuel_type
                 + highway_fuel_economy
                 + horsepower
                 + interior_color
                 + isCab
                 + is_new
                 + length
                 + make_name
                 + maximum_seating
                 + mileage
                 + owner_count
                 + salvage
                 + transmission
                 + wheel_system
                 + wheelbase
                 + width
                 + year
                 + power_rpm
                 + state,
                  direction = 'forward', trace = 0)
summary(fit_esc_4)

# Select significant model using backward BIC from fit_esc_2
# (p - 1) = 105
n = length(resid(fit_esc_2))
fit_esc_5 = step(fit_esc_2, direction = 'backward', k = log(n), trace = 0)
summary(fit_esc_5)

# Select significant model using backward BIC from fit_esc_2
# (p - 1) = 105
n = length(resid(fit_esc_2))
fit_esc_6 = step(mod_start,
                 scope = price ~ back_legroom
                 + bed_length
                 + cabin
                 + city_fuel_economy
                 + daysonmarket
                 + engine_cylinders
                 + franchise_dealer
                 + front_legroom
                 + fuel_tank_volume
                 + fuel_type
                 + highway_fuel_economy
                 + horsepower
                 + interior_color
                 + isCab
                 + is_new
                 + length
                 + make_name
                 + maximum_seating
                 + mileage
                 + owner_count
                 + salvage
                 + transmission
                 + wheel_system
                 + wheelbase
                 + width
                 + year
                 + power_rpm
                 + state, direction = 'forward', k = log(n), trace = 0)
summary(fit_esc_6)

# Select significant model using both direction AIC from fit_esc_2
# (p - 1) = 110
fit_esc_7 = step(mod_start,
                 scope = price ~ back_legroom
                 + bed_length
                 + cabin
                 + city_fuel_economy
                 + daysonmarket
                 + engine_cylinders
                 + franchise_dealer
                 + front_legroom
                 + fuel_tank_volume
                 + fuel_type
                 + highway_fuel_economy
                 + horsepower
                 + interior_color
                 + isCab
                 + is_new
                 + length
                 + make_name
                 + maximum_seating
                 + mileage
                 + owner_count
                 + salvage
                 + transmission
                 + wheel_system
                 + wheelbase
                 + width
                 + year
                 + power_rpm
                 + state, direction = 'both', trace = 0)
summary(fit_esc_7)

# Select significant model using both direction BIC from fit_esc_2
# (p - 1) = 105
fit_esc_8 = step(mod_start,
                 scope = price ~ back_legroom
                 + bed_length
                 + cabin
                 + city_fuel_economy
                 + daysonmarket
                 + engine_cylinders
                 + franchise_dealer
                 + front_legroom
                 + fuel_tank_volume
                 + fuel_type
                 + highway_fuel_economy
                 + horsepower
                 + interior_color
                 + isCab
                 + is_new
                 + length
                 + make_name
                 + maximum_seating
                 + mileage
                 + owner_count
                 + salvage
                 + transmission
                 + wheel_system
                 + wheelbase
                 + width
                 + year
                 + power_rpm
                 + state, direction = 'both', k = log(n), trace = 0)
summary(fit_esc_8)

# fit model w/ selection of significant variables from fit_esc_2
# (p - 1) = 47
fit_esc_9 = lm(price ~ back_legroom
               + bed_length
               + city_fuel_economy
               + engine_cylinders
               + franchise_dealer
               + front_legroom
               + fuel_tank_volume
               + fuel_type
               + highway_fuel_economy
               + horsepower
               + is_new
               + length
               + make_name
               + maximum_seating
               + mileage
               + owner_count
               + salvage
               + wheel_system
               + width
               + state, data = used_esc)
summary(fit_esc_9)

# fit model w/ interaction experiments
# (p - 1) = 80
fit_esc_10 = lm(price ~ back_legroom
               * front_legroom
               * maximum_seating
               + bed_length
               + city_fuel_economy
               * highway_fuel_economy
               + engine_cylinders
               + franchise_dealer
               * make_name
               + fuel_tank_volume
               * fuel_type
               + horsepower
               + is_new
               + length
               * width
               + mileage
               + owner_count
               + salvage
               + wheel_system
               + state, data = used_esc)
summary(fit_esc_10)

# fit model w/ interaction experiments
# (p - 1) = 87
fit_esc_11 = lm(price ~ back_legroom
                * front_legroom
                * maximum_seating
                + bed_length
                + city_fuel_economy
                * highway_fuel_economy
                + engine_cylinders
                * horsepower
                + franchise_dealer
                * make_name
                + fuel_tank_volume
                * fuel_type
                + is_new
                * owner_count
                * mileage
                + length
                * width
                + salvage
                + wheel_system
                + state, data = used_esc)
summary(fit_esc_11)
mean_hat_val = mean(hatvalues(fit_esc_11))
hat_vals = hatvalues(fit_esc_11)
high_lev_11 = hat_vals[hat_vals > 2 * mean_hat_val] # high leverage

# fitted vs residuals
plot(fitted(fit_esc_11), resid(fit_esc_11), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Model 11")
abline(h = 0, col = "darkorange", lwd = 2)

# Refined Model Selection for East North & South Central Regions
#-- ENC --#
# fit naive base model (using predetermined predictors)
base_region_model = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer, data = used_enc)
summary(base_region_model) # view naive model summary
calc_loocv_rmse(base_region_model) # calculate leave one out cross validation
cal_rmse(base_region_model) # calculate RMSE
# select naive base model variables
base_data = base_region_model[c("engine_displacement", "engine_cylinders", "highway_fuel_economy", "horsepower", "maximum_seating", "transmission", "mileage", "year", "wheel_system", "power_hp", "bed_length", "cabin", "salvage", "franchise_dealer")]
pairs(base_region_model) # view scatterplot matrix


### explore naive base model with state variable, (state is significant)
drops = c("region") # set field to drop
used_enc = subset(used_enc, select = -c(region)) # remove drop field(s)
# fit model
base_region_model_state = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer + state, data = used_enc)
summary(base_region_model_state) # view naive model summary
calc_loocv_rmse(base_region_model_state) # calculate leave one out cross validation
cal_rmse(base_region_model_state) # calculate RMSE
anova(base_region_model, base_region_model_state) # compare two models for significance


full_region_model = lm(price ~ ., data = used_enc) # fit full naive model (using all possible predictors)
calc_loocv_rmse(full_region_model) # calculate leave one out cross validation
cal_rmse(full_region_model) # calculate RMSE


### apply forward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
full_model_bic_forward = step(base_region_model_state, scope = price ~., direction = "forward", k = log(n)) # select model via forward BIC
summary(full_model_bic_forward) # view selected model summary
calc_loocv_rmse(full_model_bic_forward) # calculate leave one out cross validation
cal_rmse(full_model_bic_forward) # calculate RMSE


### apply backward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
region_model_bic = step(full_region_model, direction = "backward", k = log(n),trace = 0) # select model via backward BIC
summary(region_model_bic) # view selected model summary
calc_loocv_rmse(region_model_bic) # calculate leave one out cross validation
cal_rmse(region_model_bic) # calculate RMSE
car::vif(region_model_bic) # view variance inflation factors


### fit improved region model with transformed response
improved_region_model = lm(formula = log(price) ~ back_legroom  + 
                             engine_cylinders + franchise_dealer + front_legroom + fuel_tank_volume + 
                             fuel_type + height + horsepower + isCab + 
                             is_new  + make_name + maximum_seating + mileage + 
                             seller_rating  + wheel_system + wheelbase + 
                             width + year + power_rpm + torque_lb_ft + torque_rpm + state, data = used_enc) # fit model
summary(improved_region_model) # view improved model summary
calc_loocv_rmse(improved_region_model) # calculate leave one out cross validation
cal_rmse(improved_region_model) # calculate RMSE


#-- ESC --#
# fit naive base model (using predetermined predictors)
base_region_model = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer, data = used_esc)
summary(base_region_model) # view naive model summary
calc_loocv_rmse(base_region_model) # calculate leave one out cross validation
cal_rmse(base_region_model) # calculate RMSE
# select naive base model variables
base_data = base_region_model[c("engine_displacement", "engine_cylinders", "highway_fuel_economy", "horsepower", "maximum_seating", "transmission", "mileage", "year", "wheel_system", "power_hp", "bed_length", "cabin", "salvage", "franchise_dealer")]
pairs(base_region_model) # view scatterplot matrix


### explore naive base model with state variable, (state is significant)
drops = c("region") # set field to drop
base_region_model = base_region_model[ , !(names(base_region_model) %in% drops)] # remove drop field(s)
# fit model
base_region_model_state = lm(price ~ engine_displacement + engine_cylinders + highway_fuel_economy + horsepower+ maximum_seating + transmission + mileage + year + wheel_system + power_hp + bed_length + cabin + salvage + franchise_dealer + state, data = used_esc)
summary(base_region_model_state) # view naive model summary
calc_loocv_rmse(base_region_model_state) # calculate leave one out cross validation
cal_rmse(base_region_model_state) # calculate RMSE
anova(base_region_model, base_region_model_state) # compare two models for significance


full_region_model = lm(price ~ ., data = used_esc) # fit full naive model (using all possible predictors)
calc_loocv_rmse(full_region_model) # calculate leave one out cross validation
cal_rmse(full_region_model) # calculate RMSE


### apply forward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
full_model_bic_forward = step(base_region_model_state, scope = price ~., direction = "forward", k = log(n)) # select model via forward BIC
summary(full_model_bic_forward) # view selected model summary
calc_loocv_rmse(full_model_bic_forward) # calculate leave one out cross validation
cal_rmse(full_model_bic_forward) # calculate RMSE


### apply backward BIC search for improved model
n = length(resid(full_region_model)) # set number of observations
region_model_bic = step(full_region_model, direction = "backward", k = log(n),trace = 0) # select model via backward BIC
summary(region_model_bic) # view selected model summary
calc_loocv_rmse(region_model_bic) # calculate leave one out cross validation
cal_rmse(region_model_bic) # calculate RMSE
car::vif(region_model_bic) # view variance inflation factors


### fit improved region model with transformed response
improved_region_model = lm(formula = log(price) ~ back_legroom  + 
                             engine_cylinders + franchise_dealer + front_legroom + fuel_tank_volume + 
                             fuel_type + height + horsepower + isCab + 
                             is_new  + make_name + maximum_seating + mileage + 
                             seller_rating  + wheel_system + wheelbase + 
                             width + year + power_rpm + torque_lb_ft + torque_rpm + state, data = used_esc) # fit model
summary(improved_region_model) # view improved model summary
calc_loocv_rmse(improved_region_model) # calculate leave one out cross validation
cal_rmse(improved_region_model) # calculate RMSE
```

***
